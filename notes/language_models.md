# Language models
So far we have been concerned with classification problems where the input is text and the output is a categorical label.
Starting from this week, we will consider problems where the output is a sequence of symbols.
Many NLP tasks fall in this category.

Before going into sequence prediction,
let's consider the problem of density estimation,
i.e. assigning a probability to a sequence of words.
Why do we care about this problem?
Well, for applcations where the output are sentences,
e.g. speech recognition and machine translation,
we want to measure how fluent the output sentences are;
in other words, how likely that the sentences are generated by a native speaker of a certain language.

A language model assigns a probability to any sequence of words.
Let $x_1, \ldots, x_n$ be a sequence of $n$ tokens,
how should we model $p(x_1, \ldots, x_n)$?
We have already encountered a similar problem in text classification.
If we assume that $x_i$'s are independent like in Naive Bayes models,
then we have $p(x_1, \ldots, x_n) = \prod_{i=1}^np(x_i)$.
Can we do better?
Recall the chain rule of probability, we have
$$
p(x_1, \ldots, x_n) =  p(x_1) p(x_2\mid x_1) \cdots p(x_n\mid x_1, \ldots, x_{n-1}) \;.
$$
Now, we can model each conditional probability with a context of $m$ words
by a categorical distribution,
and the MLE estimate of, say $p(\text{jumped} \mid \text{the brown fox})$
is simply the count of "jumped" in our corpus divided by the count of the trigram "the brown fox".
The problem is that we will need a huge number of parameters for this model
given that the number of conext increases exponentially with the context size.
Due to the sparsity of language, we are unlikely to get enough data to estimate these parameters.

## N-gram language models
To simplify the model above,
let's use the Markov assumption:
a token only depends on $k$ previous tokens:
$$
p(x_1, \ldots, x_n) \approx \prod_{i=1}^n p(x_i\mid x_{i-k}, \ldots x_{i-1}) \;.
$$
Note that the Naive Bayes assumption corresponds to a unigram language model here.
In general, a $n$-gram model assumes that each token depends on the previous $n-1$ tokens.

The MLE estimate of the conditional probabilies are:
$$
p(x_i\mid x_{i-k}, \ldots x_{i-1}) = \frac{\text{count}(x_{i-k}, \ldots, x_{i-1}, x_i)}{\sum_{w\in V}\text{count}(x_{i-k}, \ldots, x_{i-1}, w)} \;.
$$
In words, the probability of a token following some context
is simply the fraction of times we see that token following the context
out of all tokens following the context in our corpus.
Check out the [ngram counts](https://books.google.com/ngrams) from Google Books.

**Exercise.**
Derive the MLE estimate for n-gram language models.
[Hint: Note that given a context, the conditional probabilities need to sum to one.
You can use Langrange multiplier to solve the constrained optimization problem.]

### Backoff and interpolation
In practice, what context size should we use?
Larger $n$ helps us capture long-range dependency in language,
but small $n$ allows for more accurate estimation.

**Backoff.**
One simple idea is to use larger $k$ when we have more "evidence".
For example, use the maximum $n$ where we have more than $\alpha$ counts,
and the minimum $n$ we use is 1, i.e. unigram.

**Interpolation.**
A better idea is to interpolate probabilities estimated from different n-grams instead of committing to one.
For example,
$$
p(x_i\mid x_{i-1}, x_{i-2}) = 
\lambda_1 p(x_i) +
\lambda_2 p(x_i\mid x_{i-1}) +
\lambda_3 p(x_i\mid x_{i-1}, x_{i-2})
\;.
$$
We can choose $\lambda_i$'s using cross validation.

### Smoothing
Note that the above model would assign zero probability to 
any sequence containing a word that never occurs in the training corpus,
which is obviously undersirable.
So we would like to assign probabilities to unseen words as well.

One such technique we have already seen is Laplace smoothing where we add one to each count.
It works well for text classification where we only considered unigram probabilities.
However, for longer context, many tokens are unlikely to occur.
For example, given the context "I have just", only some words (e.g. verbs) are likely to follow;
even if we have access to infinite data, the probability of certain words are close to zero.
Thus Laplace smoothing would assign too much probability mass to unseen words when the true occurences are sparse.
One quick fix is to use a pseudocount of $\alpha$ where $\alpha \lt 1$,
but the optimal value is data dependent.

Next, let's consider a better solution.
Instead of allocating a fixed amount of probability mass to the unseen words,
we can estimate the probability of an unseen word by the probability of words that we've seen once,
assuming that the frequencies of these words are similar.
For example, suppose we are drawing from an urn of balls with an unknown number of colors.
If we have drawn 3 red balls, 4 yellow balls, 1 black ball, and 1 blue ball,
then the chance that the next ball will be of an unseen color
can be estimated by the probability of black and blue balls which have occurred once,
i.e. $(1+1)/(3+4+1+1)=2/9$.

#### Good-Turing smoothing
Let's consider the problem of estimating the count of a set of objects (e.g. words in the case of language modeling),
given observations of a subset of these objects.
Let $N_r$ be the number of objects that have occurred $r$ times.


## Neural language models

## Evaluation
