# Basic machine learning

Machine learning is the main driving force of the success in NLP these days.
Instead of specifying rules to process text (e.g., translate between two specific language pairs),
which is difficult to scale,
the goal of ML is to automatically learn the model from data.

The success of ML depends on two things:

- **Availability of large amounts of data.**
Machine learning makes sense only if it is cheaper to obtain data than to write rules.
Fortunately, there are plenty of text online.
Sometimes we can directly get labels for a task, e.g., language modeling, news summarization.
Sometimes we have to crowdsource labels, e.g., text entailment, reading comprehension.
Some tasks require expert annotations, e.g., constituent parsing, semantic parsing.
A famous saying about ML systems is "garbage in, garbage out".
We should always keep in mind that there is noise and biases in the data
and ML systems are not immune to them.

- **Generalization to unseen samples.**
We are often not interested in performing well on the collected data (the training set);
it is easy to memorize all labels.
Instead, we want the model to make predictions on unseen examples (the test set).
ML promises generalization to unseen examples *from the same distribution*,
which is formalized in statistical learnign theory.
In practice, however, the test examples (generated by end users) are rarely from the training distribution.
Therefore, we should be aware of the limit of learning.

# Modeling, learning, inference
