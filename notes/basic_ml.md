# Basic machine learning

Machine learning is the main driving force of the success in NLP these days.
Instead of specifying rules to process text (e.g., translate between two specific language pairs),
which is difficult to scale,
the goal of ML is to automatically learn the "rules" from data.

The success of ML depends on two things:

- **Availability of large amounts of data.**
Machine learning makes sense only if it is cheaper to obtain data than to write rules.
Fortunately, there are plenty of text online, although sometimes we need to be smart about getting labels.
    - Tasks where we can get labels for free: language modeling, news summarization etc.
    - Tasks where we need to crowdsource labels: text entailment, reading comprehension etc.
    - Tasks that require expert annotations: constituent parsing, semantic parsing etc.
A famous saying about ML is "garbage in, garbage out": you only get results as good as your data.
We should always keep in mind that there is noise and biases in the data
and ML systems are not immune to them.

- **Generalization to unseen samples.**
We are often not interested in performing well on the collected data (the training set);
after all it is easy to memorize all labels.
Instead, we want the model to make predictions on *unseen* inputs (the test set).
ML promises generalization to unseen examples *from the same distribution*.
This is formalized in statistical learnign theory.
In practice, however, the test examples (generated by end users) are rarely from the training data distribution.
Therefore, we should be aware of the limit of learning.

## Modeling, learning, inference
The ML approach to any task consists of the following three components: modeling, learning, and inference.

### Modeling
The goal of modeling is to represent the task by a mathematical object that we can manipulate.
Generally, for NLP problems, given an input space $\mathcal{X}$ and an output space $\mathcal{Y}$,
the model is a function $f\colon \mathcal{X} \rightarrow \mathcal{Y}$.
Take sentiment classification as an example:
we have $\mathcal{X}=\{\text{sentences of interest}\}$ and $\mathcal{Y}=\{\text{negative}, \text{positive}\}$.

The modeling question here is what is $f$.
One possible solution is to use a lookup table---we save the value of $y\in\mathcal{Y}$ for every possible input.
What is the problem with this model?
Well, it has a huge size (number of parameters) and will be difficult to learn.
Instead of taking the complete sentence as input, we might want to just consider individual words in it (i.e the bag-of-words model).

Now, obviously the bag of words model breaks the sentence structure and we lose information in this process (it's not possible to get back the exact sentence given individual words in it).
However, the point of modeling is to capture the essential relations in a managable form, and anything else that is not modeled is considered as "noise".
In this course we will go through different modeling techniques for text, e.g. log linear models and neural networks.

### Learning
Specifiying the model is often not enough because it may depend on unknown parameters.
Learning allows us to estimate these parameters from data.
Typically, learning consists of two steps: (a) specify a loss function (e.g. squared loss),
and (b) minimize the average loss (i.e. empirical risk minimization).
The optimization step (b) is often done by stochastic gradient descent.

The key topic in most learning algorithms is **generalization**:
If we minimize the loss on the training data, how do we know that the model will have low loss on the test data too?
The situation we want to avoid is **overfitting**,
where the model performs very well on the training data but poorly on the test data.
To prevent overfitting, we often want to make our model family simple because a complex function is more likely to memorize the training examples (think a lookup table).
There are many ways to restrict the model family depending on the specific model at hand.

### Inference
Given a model with learned parameters, we still need to execute the model on some input to obtain the output.
This process is called inference (sometimes refered to as decoding or the argmax problem).
It may seem trivial at first.
In the sentiment classification example, we can simly compute the classifier score for each class and predict the highest-scoring one, which takes $O(\text{number of classes})$.
However, inference becomes slightly more complex if our output space contains sequences or trees,
because there are exponentially many sequences or trees and enumerating their scores is intractable.
We will need dynamic programming or integer linear programming in these cases.
In latent variable models, we might want to marginalize over the latent variables,
which again can be intractable.
The main point is that inference is not often as easy as taking class with the maximum score.

