\documentclass[usenames,dvipsnames,notes,11pt,aspectratio=169]{beamer}
\usepackage{ifthen}
\usepackage{xcolor}
\usepackage{pgfplots}
\usepackage{amsmath}
\usepackage{centernot}
\usepackage{xspace}
\usepackage{pifont}
\usepackage{tabularx}
\usepackage{makecell}
\usepackage{cuted}
\usepackage{booktabs}
\usepackage{array}
\usepackage{CJKutf8}
\usepackage{textcomp}
\usepackage{setspace}
\usepackage{pdfcomment}
\newcommand{\pdfnote}[1]{\marginnote{\pdfcomment[icon=note]{#1}}}
%\newcommand{\pdfnote}[1]{}

\usepackage{pgfpages}
%\setbeameroption{show notes on second screen}

\newcommand\w[1]{\textit{#1}}


\input ../beamer-style
\input ../std-macros
\input ../macros

\AtBeginSection[]
{
    \begin{frame}
        \frametitle{Table of Contents}
        \tableofcontents[currentsection]
    \end{frame}
}
\parskip=10pt

\title[CSCI-GA.2590]{Text Classification}
\author[He He]{He He
}
\institute[NYU]{New York University}
\date{September 15, 2021}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
    {Text classification}
    \begin{itemize}
        \item Input: text (sentence, paragraph, document)
        \item Predict the \blue{category or property} of the input text
            \begin{itemize}
                \item Sentiment classification: Is the review positive or negative?
                \item Spam detection: Is the email/message spam or not?
                \item Hate speech detection: Is the tweet/post toxic or not?
                \item Stance classification: Is the opinion liberal or conservative?
            \end{itemize}
            \pause
        \item Predict the \blue{relation} of two pieces of text
            \begin{itemize}
                \item Textual entailment (HW1): does the premise entail the hypothesis?\\
                    Premise: The dogs are running in the park.\\
                    Hypothesis: There are dogs in the park.
                \item Paraphrase detection: are the two sentences paraphrases?\\
                    Sentence 1: The dogs are in the park.\\
                    Sentence 2: There are dogs in the park.
            \end{itemize}
    \end{itemize}
    \pdfnote{
        Paraphrase detection itself may not always be a well-defined problem.
        Consider the given example, are they paraphrase?
        Well, it depends on the context.
        If the task is image caption, then yes.
        If the task is dialogue response generation, where the context is ``Where are the dogs?'',
        then S1 and S2 convey very different answers to the question.
    }
\end{frame}

\section{Generative models: naive Bayes}

\begin{frame}
    {Intuition}
    \begin{itemize}
        \itemsep1em
        \item[]\beamerblue{Example}: sentiment classification for movie reviews
    \begin{itemize}
        \item[] {\small \setstretch{0.8} \textit{Action. Comedy. Suspense. This movie has it
all. The Plot goes that 4 would be
professional thieves are invited to take part in a
heist in a small town in Montana. every type of
crime movie archetype character is here. Frank,
the master mind. Carlos, the weapons expert. Max,
the explosives expert. Nick, the safe cracker and
Ray, the car man. Our 4 characters meet up at the train station
and from the beginning none of them like or trust
one another. Added to the mix is the fact that
Frank is gone and they are not sure why they have
called together. Now Frank is being
taken back to New Jersey by the 2 detectives but
soon escapes on foot and tries to make his way
back to the guys who are having all sorts of
            problems of their own. \textcolor<2->{blue}{Truly a great
film loaded with laughs and great acting. Just an
overall good movie for anyone looking for a laugh
            or something a little different}}\par}
    \end{itemize}
    \pause
\item[]\beamerblue{Idea}: count the number of positive/negative words
    \begin{itemize}
        \item What is a ``word''?
        \item How do we know which are positive/negative?
    \end{itemize}

    \end{itemize}
    \pdfnote{
        How would you quickly tell the sentiment of this review?
        Understand everything said in it is hard (genre, plot, actor performance etc.).
        But sometimes a couple of keywords or a concluding sentence is sufficient.
    }
    \pdfnote{
        Now there are two questions left.
        We know what's a word intuitively, but to the computer the input is just a string of unicodes, how can we separate that into a list of words.
        The second question is how can we tell which words are positive or negative. The rule based approach is to construct a dictionary of such words, which can be quite effective.
        But here we'll see how to learn this from labeled data.
    }
\end{frame}

\begin{frame}
    {Preprocessing: tokenization}
    \beamerblue{Goal}: Splitting a string of text $s$ to a sequence of \textbf{tokens} $[x_1, \ldots, x_n]$.

    \beamerblue{Language-specific solutions}\\
    \begin{itemize}
        \itemsep1em
        \item Regular expression: ``I didn't watch the movie''. $\rightarrow$ [``I'', ``did'', ``n't'', ``watch'', ``the'', ``movie'', ``.''] 
            \begin{itemize}
                \item Special cases: U.S., Ph.D. etc.
            \end{itemize}
        \item Dictionary / sequence labeler: 
            \begin{CJK*}{UTF8}{gbsn}
                ``我没有去看电影。'' $\rightarrow$ [``我'', ``没有'', ``去'', ``看'', ``电影'', ``。'']
            \end{CJK*}
    \end{itemize}

    \pause
    \beamerblue{General solutions}: don't split by words\\
    \begin{itemize}
        \item Characters:
            [``u'', ``n'', ``a'', ``f'', ``f'', ``a'', ``b'', ``l'', ``e''] (pros and cons?)
            \pause
        \item Subword (\eg byte pair encoding):
            [``un'', ``aff'', ``able\#''] \gray{[board]} 
    \end{itemize}
    \pdfnote{
        Note that for contractions like didn't. We can tokenize it into either did n't or didn 't. Both are okay as long as it's consistent.
        English tokenization gets more complex when there is punctuations or special symbols.
    }
    \pdfnote{Tokenization can have important impact on the performance of downstream learning algorithms.
    }
    \pdfnote{
        Using character sequences (or even byte sequences) we impose mininal prior knowledge on what is a word. Given enough data, the model can probably figure out a reasonable unit of the characters based on their frequencies. But one downside in this approach is that the sequence is now much longer, and the computation time of many algorithms grows with sequence length, which will be expensive for large-scale training.
    }
    \pdfnote{
        A middle ground is to use subword, a unit larger than characters but smaller than words.
        This is commonly used in large-scale models nowadays.
        The BPE algorithms is a simple technique from data compression.
        The basic idea is to replace commonly occured sequences with another symbol.
    }
\end{frame}

\begin{frame}
    {Classification: problem formulation}
    \begin{itemize}
        \itemsep1em
        \item \beamerblue{Input}: a sequence of tokens $x=(x_1, \ldots x_n)$ where $x_i \in \mathcal{V}$.
        \item \beamerblue{Output}: binary label $y\in \pc{0, 1}$.
        \item \beamerblue{Probabilistic model}:
            $$
            f(x) = \begin{cases}
 1 & \text{if $p_\theta(y=1\mid x) > 0.5$} \\
 0 & \text{otherwise}
 \end{cases} ,
            $$
            where $p_\theta$ is a distribution parametrized by $\theta\in{\Theta}$.
        \item Question: how to choose $p_\theta$?
    \end{itemize}
    \pdfnote{
        Choosing $p_\theta$ is the modeling part where our task-specific knowledge comes in: how should the label depend on the text.
    }
\end{frame}

\begin{frame}
    {Model $p(y\mid x)$}
    How to write a review:\\
    \begin{enumerate}
        \item Decide the sentiment by flipping a coin: $p(y)$
        \item Generate word sequentially conditioned on the sentiment  $p(x\mid y)$
    \end{enumerate}
    \pause
    \vspace{-1ex}
    \onslide<+->{
        \begin{align}
            p(y)&=\onslide<+->{\text{Bernoulli}(\alpha)} \\
            p(x\mid y)&=\onslide<+->{\prod_{i=1}^n p(x_i\mid y)\quad\quad \text{(independent assumption)}}\\
                &=\onslide<+->{\prod_{i=1}^n \text{Categorical}(
                \underbrace{\theta_{1,y},\ldots,\theta_{|\sV|,y}}_{\text{sum to 1}}
                )}
        \end{align}
    }
    \vspace{-1ex}
    \onslide<+->{
    \begin{block}
        {Bayes rule}
        $$
        p(y\mid x) = \frac{p(x\mid y)p(y)}{p(x)}
        = \frac{p(x\mid y)p(y)}{\sum_{y\in\mathcal{Y}} p(x\mid y)p(y)}
        $$
    \end{block}
}
\end{frame}

\begin{frame}
    {Naive Bayes models}
    \begin{block}
    {Naive Bayes assumption}
        The input features are \textbf{conditionally independent} given the label:
        $$
        p(x\mid y) = \prod_{i=1}^n p(x_i\mid y) \;.
        $$
    \end{block}
    \begin{itemize}
        \item A strong assumption, but works surprisingly well in practice.

        \item $p(x_i\mid y)$ doesn't have to be a categorical distribution (\eg Gaussian distribution)
    \end{itemize}

    \beamerblue{Inference}: $y=\argmax_{y\in\sY} p_\theta(y\mid x)$ \gray{[board]}
    \pdfnote{
        Now that we have figured out the modeling and inference problem, the next problem is learning the parameters.
    }
\end{frame}

\begin{frame}
    {Maximum likelihood estimation}
    \beamerblue{Task}: estimate parameters $\theta$ of a distribution $p(y; \theta)$ given i.i.d. samples $D=\p{y_1, \ldots, y_N}$ from the distribution.

    \beamerblue{Goal}: find the parameters that make the observed data most probable.

    \pause
    \textbf{Likelihood function} of $\theta$ given $D$:
    $$
    L(\theta; D) \eqdef p(D;\theta) = \prod_{i=1}^N p(y_i; \theta) \;.
    $$

    \textbf{Maximum likelihood estimator}:
    \begin{align}
        \hat{\theta} &= \argmax_{\theta\in\Theta} L(\theta; D)
        = \argmax_{\theta\in\Theta} \sum_{i=1}^N \log p(y_i; \theta)
    \end{align}

    \pdfnote{To make ``most probable'' more precise, we define the likelihood function of the parameters to be the probability of the data given by the model.}
    \pdfnote{Why can we write the joint distribution as the product? Independent assumption from iid.}
    \pdfnote{Now it's reduced to an optimization problem.}
\end{frame}

\begin{frame}
    {MLE and ERM}
    ERM:
    $$
    \min \sum_{i=1}^N \ell(x^{(i)}, y^{(i)}, \theta)
    $$
    \pause

    MLE:
    $$
    \max \sum_{i=1}^N \log p(y^{(i)} \mid x^{(i)}; \theta)
    $$
    \pause

    What's the connection between MLE and ERM?

    MLE is equivalent to ERM with the \textbf{negative log-likelihood} (NLL) loss function:
    $$
    \ell_{\text{NLL}}(x^{(i)}, y^{(i)}, \theta) \eqdef -\log p(y^{(i)} \mid x^{(i)}; \theta)
    $$
\end{frame}

\begin{frame}
    {MLE for our Naive Bayes model}
    \gray{[board]}
\end{frame}

\begin{frame}
    {MLE solution for our Naive Bayes model}
    \begin{align*}
        \text{count}(w, y) &\eqdef \text{frequency of $w$ in documents with label $y$}\\ 
        p_{\text{MLE}}(w\mid y) &= \frac{\text{count}(w, y)}{\sum_{w\in\mathcal{V}}\text{count}(w, y)}\\
        &= \text{how often the word occur in positive/negative documents}\\
        p_{\text{MLE}}(y=k) &= \frac{\sum_{i=1}^N \1\p{y^{(i)}=k}}{N}\\
        &= \text{fraction of positive/negative documents}
    \end{align*}
    \pause
    \textbf{Smoothing}: reserve probability mass for unseen words
    $$
        p(w\mid y) = \frac{{\color{blue}\alpha} + \text{count}(w, y)}{\sum_{w\in\mathcal{V}}\text{count}(w, y) + {\color{blue}\alpha|\mathcal{V}|}}
    $$
    Laplace smoothing: $\alpha=1$
    \pdfnote{
        How well would this model generalize to unseen documents?
        What if we have a word that's not seen during training?
    }
\end{frame}

\begin{frame}
    {Feature design}
        Naive Bayes doesn't have to use single words as features
    \begin{itemize}
        \itemsep1em
        \item Lexicons, \eg LIWC.
        \item Task-specific features, \eg is the email subject all caps.
        \item Bytes and characters, \eg used in language ID detection.
        %\item $n$-grams (a symbol of $n$ consecutive tokens)
    \end{itemize}
    \pdfnote{
        Char/byte NB model is a very fast and effective language ID detector (e.g., google translate).
    }
\end{frame}

\begin{frame}
    {Summary of Naive Bayes models}
    \begin{itemize}
        \item Modeling: the conditional indepedence assumption simplifies the problem
        \item Learning: MLE (or ERM with negative log-likelihood loss)
        \item Inference: very fast (adding up scores of each word)
    \end{itemize}
\end{frame}

\section{Discriminative models: logistic regression}

\begin{frame}
    {Discriminative models}
    % TODO: move this slides to later
    \beamerblue{Idea}: directly model the conditional distribution $p(y\mid x)$
    \pause
    \begin{table}
        \renewcommand{\arraystretch}{1.5}
        \begin{tabular}{lcc}
            & generative models & discriminative models \\
            \hline
            modeling & joint: $p(x,y)$ & conditional: $p(y\mid x)$ \\
            assumption on $y$ & yes & yes \\
            assumption on $x$ & yes & no \\
            development & generative story & feature extractor
        \end{tabular}
    \end{table}
    \pdfnote{
        In Naive Bayes model, we used Bayes rule to get $p(y\mid x)$ given $p(x\mid y)$ and the class prior $p(y)$.
        But one question here is, if $p(y\mid x)$ is what we are ultimately interested in, why bother modeling the data likelihood and the prior as opposed to directly modeling $p(y\mid x)$.
    }
\end{frame}

\begin{frame}
    {Model $p(y\mid x)$}
    How to model $p(y\mid x)$?
    \begin{itemize}
        \item[] $y$ is a Bernoulli variable:
            $$
            p(y\mid x) = \alpha^y (1-\alpha)^{(1-y)}
            $$
            \pause
        \item[] Bring in $x$:
            $$
            p(y\mid x) = h(x)^y (1-h(x))^{(1-y)} \quad h(x) \in [0,1]
            $$
    \end{itemize}

    \pause
    Parametrize $h(x)$ using a linear function:\\
    $$
    h(x) = w\cdot \phi(x) + b \quad\quad \phi\colon \sX \rightarrow \BR^d
    $$

    Problem: $h(x) \in \BR$ (score)
\end{frame}

\begin{frame}
    {Logistic regression}
    Map $w\cdot\phi(x) \in\mathbb{R}$ to a probability by the \textbf{logistic function}
    \vspace{-1em}
    \begin{center}
        \includegraphics[height=4cm]{figures/logistic}
    \end{center}
    \vspace{-2em}
    \begin{align*}
    p(y=1\mid x; w) &= \frac{1}{1 + e^{-w\cdot\phi(x)}} \quad (y\in \pc{0,1})\\
        p(y=k\mid x; w) &= \frac{e^{w_k\cdot\phi(x)}}{\sum_{i\in\mathcal{Y}}e^{w_i\cdot\phi(x)}} \quad (y\in \pc{1, \ldots, K}) && \text{``softmax''}
    \end{align*}
    \pdfnote{
        Note that in multiclass classification setting, there is one $w$ for each class.
    }
\end{frame}

\begin{frame}
    {Inference}
    \begin{align}
        \hat{y} &= \argmax_{k\in\sY} p(y=k\mid x; w) \\
        &= \argmax_{k\in\sY} \frac{e^{w_k\cdot\phi(x)}}{\sum_{i\in\mathcal{Y}}e^{w_i\cdot\phi(x)}} \\
        &= \argmax_{k\in\sY} e^{w_k\cdot\phi(x)} \\
        &= \argmax_{k\in\sY} \underbrace{w_k\cdot\phi(x)}_{\textstyle\text{score for class $k$}}
    \end{align}
\end{frame}

\begin{frame}
    {MLE for logistic regression}
    \gray{[board]}
    \pause
    \begin{itemize}
        \item Likelihood function is concave
        \item No closed-form solution 
        \item Use gradient ascent 
    \end{itemize}
    \pdfnote{LR is probabilisti, so we can still do MLE.}
\end{frame}

\begin{frame}
    {BoW representation}
    \textbf{Feature extractor}: $\phi\colon \mathcal{V}^n \rightarrow \mathbb{R}^d$.

    \beamerblue{Idea}: a sentence is the ``sum'' of words.

    Example:
    \begin{align*}
        \mathcal{V} &= \pc{\w{the}, \w{a}, \w{an}, \w{in}, \w{for}, \w{penny}, \w{pound}}\\
        \text{sentence} &= \w{in for a penny, in for a pound} \\
        x &= \p{\w{in}, \w{for}, \w{a}, \w{penny}, \w{in}, \w{for}, \w{a}, \w{pound}}
    \end{align*}

    $$
    \phi_{\text{BoW}}(x) = \sum_{i=1}^n \phi_{\text{one-hot}}(x_i)
    $$

    \gray{[board]}
\end{frame}

\begin{frame}
    {Compare with naive Bayes}
    \begin{itemize}
        \itemsep2em
        \item Our naive Bayes model ($x_i \in \pc{1, \ldots, |\sV|}$):
            $$
            X_i \mid Y=y \sim \text{Categorical}(\theta_{1,y}, \ldots, \theta_{|\sV|, y}) \;.
            $$
        \item The naive Bayes generative story produces a BoW vector following a multinomial distribution:
            $$
            \phi_{\text{BoW}}(X) \mid Y=y \sim \text{Multinomial}(\theta_{1,y}, \ldots, \theta_{|\sV|,y}, n) \;.
            $$

        \pause
        \item Both multinomial naive Bayes and logistic regression learn a linear separator $w\cdot\phi_{\text{BoW}}(x)+b=0$.
    \end{itemize}
    \pause
    Question: what's the advantage of using logistic regression?

    \pdfnote{
        In this sense, NB is trying to model the BoW feature vector.
    }
    \pdfnote{
        In addition, they both learn a linear predictor which simply sums the score of each word at inference time.
        For NB, $w$ is $\log \theta$.
    }
\end{frame}

\begin{frame}
    {Feature extractor}
    Logistic regression allows for richer features.

    Define each feature as a function $\phi_i\colon \mathcal{X} \rightarrow \BR$.
    \begin{align*}
 \phi_1(x) &= \begin{cases}
 1 & \text{$x$ contains ``happy''} \\
 0 & \text{otherwise}
 \end{cases} ,
 \\
 \phi_2(x) &= \begin{cases}
 1 & \text{$x$ contains words with suffix ``yyyy''} \\
 0 & \text{otherwise}
 \end{cases} .
    \end{align*}
    In practice, use a dictionary
    $$
    \texttt{feature\_vector}[\texttt{"prefix=un+suffix=ing"}] = 1
    $$
    \pdfnote{
        With NB, we can still include these features as variables, but we'll have to think about modeling them as a parametrized distribution and handling the sparsity problem during estimation.
    }
\end{frame}

\begin{frame}
    {Feature vectors for multiclass classification}
    Multinomial logistic regression
    \begin{tikzpicture}
        \node (a) {
            \begin{minipage}{\textwidth}
            \begin{align*}
        p(y=k\mid x; w) =
             \frac{e^{{\color{blue}w_k\cdot\phi(x)}}}
                {\sum_{i\in\mathcal{Y}}e^{w_i\cdot\phi(x)}} \quad (y\in \pc{1, \ldots, K})
            \end{align*}
            \end{minipage}
};
        \node(b) [below= of a] {
            \begin{minipage}{\textwidth}
    $$
        p(y=k\mid x; w) =
                \frac{e^{{\color{blue}w\cdot\Psi(x, k)}}}{\sum_{i\in\mathcal{Y}}e^{w\cdot\Psi(x, i)}} \quad (y\in \pc{1, \ldots, K})
    $$
            \end{minipage}
};
        \draw[arrow] (a) -- (b);
    \end{tikzpicture}
    scores of each class $\rightarrow$ compatibility of an input and a label

    \pause
    Multivector construction of $\Psi(x,y)$:
    $$
    \Psi(x, 1) \eqdef \pb{\underbrace{0}_{\textstyle y=0}, \underbrace{\phi(x)}_{\textstyle y=1}, \ldots, \underbrace{0}_{\textstyle y=K}}
    $$
\end{frame}

\begin{frame}
    {N-gram features}
    Potential problems with the the BoW representation?

    \pause
    \textbf{N-gram} features:
    \begin{center}
        \textit{in for a penny , in for a pound}
    \end{center}
    \begin{itemize}
        \item Unigram: in, for, a, ...
        \item Bigram: in/for, for/a, a/penny, ...
        \item Trigram: in/for/a, for/a/penny, ...
    \end{itemize}

    What's the pros/cons of using higher order n-grams?
    \pdfnote{
        BoW problem: new york, don't like
    }
\end{frame}

\section{Regularization, model selection, evaluation}

\begin{frame}
    {Error decomposition}
    (ignoring optimization error)

    \begin{align*}
        \text{risk}(h) - \text{risk}(h^*) = \text{approximation error} + \text{estimation error}
    \end{align*}
    \gray{[board]}

    \pause
    Larger hypothesis class: approximation error $\downarrow$, estimation error $\uparrow$

    Smaller hypothesis class: approximation error $\uparrow$, estimation error $\downarrow$

    How to control the size of the hypothesis class?
\end{frame}

\begin{frame}
    {Reduce the dimensionality}
    Linear predictors: $\sH = \pc{w : w \in \BR^d}$

    Reduce the number of features:\\
    \gray{[discussion]}

    \pause
    Other predictors:\\
    \begin{itemize}
        \item Depth of decision trees
        \item Degree of polynomials
        \item Number of decision stumps in boosting
    \end{itemize}
    \pdfnote{
        stopwords, stemming, filter by frequency
    }
    \pdfnote{
        feature selection (fwd/bwd), L1
    }
\end{frame}

\begin{frame}
    {Regularization}
    Reduce the ``size'' of $w$:
    $$
    \min_w \underbrace{\frac{1}{N}\sum_{i=1}^N L(x^{(i)}, y^{(i)}, w)}_{\textstyle \text{average loss}}
    + \underbrace{\frac{\lambda}{2}\|w\|_2^2}_{\textstyle \ell_2 \text{ norm}}
    $$

    Why is small norm good? Small change in the input doesn't cause large change in the output.

    \gray{[board]}
\end{frame}

\begin{frame}
    {Gradient descent with $\ell_2$ regularization}
    Run SGD on 
    $$
    \min_w \underbrace{\frac{1}{N}\sum_{i=1}^N L(x^{(i)}, y^{(i)}, w)}_{\text{average loss}}
    + \underbrace{\frac{\lambda}{2}\|w\|_2^2}_{\ell_2 \text{ norm}}
    $$

    Also called \textbf{weight decay} in the deep learning literature:
    $$
    w \leftarrow w - \eta(\nabla_w L(x, y, w) + {\color{blue}\lambda w})
    $$

    Shrink $w$ in each update.
\end{frame}

\begin{frame}
    {Hyperparameter tuning}
    \textbf{Hyperparameters}: parameters of the learning algorithm (not the model)

    Example: use MLE to learn a logistic regression model using BoW features \\
    \bigskip
    \pause

    \pause
    How do we select hyperparameters?\\
    \begin{itemize}
        \item[] Pick those minimizing the training error?
        \item[] Pick those minimizing the test error?
    \end{itemize}
    \pdfnote{What are the hyperparams in this case?}
    \pdfnote{
        Dillema: training error overfit. test error don't know.
    }
\end{frame}

\begin{frame}
    {Validation}
    \textbf{Validation set}: a subset of the training data reserved for tuning the learning algorithm (also called the \textbf{development set}).

    \textbf{$K$-fold cross validation}\\
    \gray{[board]}

    \pause
    It's important to look at the data and errors during development, but \textcolor{red}{not the test set}.
\end{frame}

\end{document}
