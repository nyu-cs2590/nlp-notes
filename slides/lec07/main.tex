\documentclass[usenames,dvipsnames,notes,11pt,aspectratio=169]{beamer}
\usepackage{ifthen}
\usepackage{xcolor}
\usepackage{pgfplots}
\usepackage{amsmath}
\usepackage{centernot}
\usepackage{pifont}
\usepackage{tabularx}
\usepackage{makecell}
\usepackage{cuted}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{array}
\usepackage{textcomp}
\usepackage{setspace}
\usepackage{xspace}
\usepackage{tikz}
\usepackage{pdfcomment}
\newcommand{\pdfnote}[1]{\marginnote{\pdfcomment[icon=note]{#1}}}
%\newcommand{\pdfnote}[1]{}

\usepackage{pgfpages}
%\setbeameroption{show notes on second screen}


\input ../beamer-style
\input ../std-macros
\input ../macros

\AtBeginSection[]
{
    \begin{frame}
        \frametitle{Table of Contents}
        \tableofcontents[currentsection]
    \end{frame}
}
\parskip=10pt

\title[CSCI-GA.2590]{Context-Free Parsing}
\author[He He]{He He
}
\institute[NYU]{New York University}
\date{October 27, 2021}

\begin{document}
\begin{frame}
\titlepage
\end{frame}

\begin{frame}
    {Logistics}
    \begin{itemize}
        \item Homework 3 released
        \item Project proposal due next week (one page) 
            \begin{itemize}
                \item What problem are you tackling and why is it important?
                \item What's your approach? 
                \item How do you plan to evaluate it? 
            \end{itemize}
    \end{itemize}
\end{frame}

\section{Context-free language}

\begin{frame}
    {Langauge is a set of strings}
    \textbf{Formal language}:\\
    \begin{itemize}
        \item A set of \textbf{strings} consisting of \textbf{words} from an \textbf{alphabet}
        \item \emph{Well-formed} according to a set of rules 
        \item Studies the \emph{syntactical} aspects of a language
    \end{itemize}

    Examples:\\
    \begin{itemize}
        \item Formulas (logic): $(p_1 \wedge p_2) \vee (\urcorner p_3)$
        \item Programming languages: \texttt{int a, b = 0;}
        \item Sequences from the alphabet $\pc{a,b}$ that ends with two $a$'s
    \end{itemize}

    Questions:\\
    \begin{itemize}
        \item Formal language theory: How to describe languages (expressive power, recognizability etc.) 
        \item Linguistics: Can we design formal languages that capture syntactic properties of natural language? 
    \end{itemize}

    \pdfnote{
        So far we have taken a statistical approach to language.
        The techniques we have learned is not specific to natural language and apply to any sequences of symbols, e.g. musical notes, objects in knowledge bases etc.
        So what really is language?
        Today we'll analyze language through the lens of formal language theory.
        Formal languages are designed for specific applications.
    }
    \pdfnote{
        What is the key difference between these formal language and natural language?
    }
    \pdfnote{
        Expressive power: how many different languages can it describe?
        Recognizability: how difficult it is to decide if a string belongs to a language?
    }
\end{frame}

\begin{frame}
    {Natural language syntax}
    Construct a formal language to represent the syntax of natural language\\
    \begin{itemize}
        \item \emph{Expressivity}: how many syntactic phenomena can it cover?
        \item \emph{Computation}: how fast can we parse a sentence?
    \end{itemize}

    Context-free grammars for natural language\\
    \begin{itemize}
        \item Captures nested structures which are common in natural language 
            \begin{itemize}
                \item[] [I told Mary that [John told Jane that [Ted told Tom a secret]]].
            \end{itemize}
        \item Captures long-range dependencies
            \begin{itemize}
                \item[] \emph{the} burnt and badly-ground Italian \emph{coffee}
                \item[] \emph{these} burnt and badly-ground Italian \emph{coffees}
            \end{itemize}
        \item Strikes a good balance between expressivity and computation
    \end{itemize}
\end{frame}

\begin{frame}
    {Context-free language}
    \textbf{Context-free languages (CFL)} are generated by a \textbf{context-free grammar} $G=(\Sigma, N, R, S)$:\\
    \begin{itemize}
        \item a finite alphabet $\Sigma$ of \textbf{terminals} (words)
        \item a finite set of \textbf{non-terminals} $N$ disjoint from $\Sigma$ (word groups)
        \item a set of \textbf{production rules} $R$ of the form $A\rightarrow\beta$,
            where $A \in N, \beta \in (\Sigma \cup N)^*$ (how to group words)
        \item a start symbol $S\in N$ (root of derivation)
    \end{itemize}

    Example:\\
    \begin{itemize}
        \item[] $S\rightarrow SS$
        \item[] $S\rightarrow (S)$
        \item[] $S\rightarrow ()$
    \end{itemize}
\end{frame}

\begin{frame}
    {Phrase-structure grammar for English}
    Sentences are broken down into \textbf{constituents}.

    A constituent works as a single unit in a sentence.\\
    \begin{itemize}
        \item Can be moved around or replaced without breaking grammaticality.
        \item[] (Abigail) and (her younger brother) (bought a fish).
    \end{itemize}

    Construct CFG for English\\
    \begin{itemize}
        \item Each word is a terminal, derived from its POS tag.
        \item Each sentence is derived from the start symbol $S$.
        \item Each phrase type is a non-terminal.
        \item Each constituent is derived from a non-terminal.
    \end{itemize}

    Grammar design: choose the right set of non-terminals that produces different constituents.
\end{frame}

\begin{frame}
    {A toy example CFG}
    \begin{figure}    
        \includegraphics[height=6cm]{figures/toy-cfg.png}
    \end{figure}   
    \vspace{-2em}
    \textbf{Lexicon}: rules that produce the terminals

    (Example from Mike Collins' notes)
\end{frame}

\begin{frame}
    {Parsing}
    \begin{figure}    
        \includegraphics[height=4cm]{figures/toy-cfg-2.png}
    \end{figure}  
    \vspace{-2em}
    Can we derive the sentence ``the man sleeps''?
    \vspace{9em}
\end{frame}

\begin{frame}
    {Ambiguity}
    Can a sentence have multiple parse trees?
    \begin{figure}
        \begin{subfigure}[b]{0.45\textwidth}
            \includegraphics[width=\textwidth]{figures/parse-1.png}
        \end{subfigure}
        \begin{subfigure}[b]{0.45\textwidth}
            \includegraphics[width=\textwidth]{figures/parse-2.png}
        \end{subfigure}
    \end{figure}

    Exercise: find parse trees for\\
    ``She announced a program to promote safety in trucks and vans''.
\end{frame}

\section{Probabilistic context-free grammars}

\begin{frame}
    {PCFG}
    Notation: let $\sT_G$ be the set of all possible left-most parse trees under the grammar $G$.

    Goal: define a probability distribution $p(t)$ over parse trees $t\in\sT_G$

    Parsing: pick the most likely parse tree for a sentence $s$
    $$
    \argmax_{t\in\sT_G(s)} p(t)
    $$

    Three questions:\\
    \begin{itemize}
        \item Modeling: how to define $p(t)$ for trees?
        \item Learning: how to estimate parameters of the distribution $p(t)$?
        \item Inference: how to find the most likely tree efficiently?
    \end{itemize}
    \pdfnote{
        Why do we need probabilities?
        Now, we have seen that just having the grammar is not enough,
        we can have multiple parse trees for a sentence.
        So we need to figure out which one is more likely,
        which is why we want to assign probabilities to different parse trees.
    }
\end{frame}

\begin{frame}
    {Modeling}
    Generate parse trees: iteratively sample a production rule to expand a non-terminal
    \vspace{-1em}
    \begin{figure}    
        \includegraphics[height=4cm]{figures/toy-cfg-2.png}
    \end{figure}  
    \vspace{7em}
    \pdfnote{
        Why letf-most derivation? If we have a rule X -> X X, then apply rule X -> Y Z. If we don't specifiy the order of derivation, we can have different trees given the same sequence of rules.
    }
\end{frame}

\begin{frame}
    {PCFG}
    A \textbf{PCFG} consists of\\
    \begin{itemize}
        \item A CFG $G=(\Sigma, N, R, S)$
        \item Probabilities of production rules $q(\alpha\rightarrow\beta)$ for each $\alpha\rightarrow\beta\in R$ such that
            $$
            \sum_{\beta\colon X\rightarrow\beta\in R}q(X\rightarrow\beta) = 1
            \quad \forall X\in N
            $$
    \end{itemize}
    \vspace{-1em}
    \begin{figure}    
        \includegraphics[height=4cm]{figures/toy-pcfg.png}
    \end{figure} 
    \pdfnote{
        Sum of rules with the same LHS is 1.
    }
\end{frame}

\begin{frame}
    {From HMM to PCFG}
\end{frame}

\begin{frame}
    {Probabilities of parse trees}
    Given a parse tree $t$ consisting of rules
    $\alpha_1\rightarrow\beta_1,\ldots, \alpha_n\rightarrow\beta_n$,
    its probabilities under the PCFG is
    $$
    p(t) = \prod_{i=1}^n q(\alpha_i\rightarrow\beta_i)
    $$
    \vspace{-1em}
    Example:

    \begin{columns}
        \begin{column}{0.5\textwidth}
    \begin{figure}    
        \includegraphics[height=3cm]{figures/toy-tree.png}
    \end{figure}  
        \end{column}
        \begin{column}{0.5\textwidth}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    {Learning}
    Training data: treebanks
    \vspace{-1em}
    \begin{figure}    
        \includegraphics[height=4cm]{figures/treebank.png}
    \end{figure}  
    Given a set of trees (production rules),
    we can estimate rule probabilities by MLE.
    $$
    q(\alpha\rightarrow\beta) = \frac{\text{count}(\alpha\rightarrow\beta)}{\sum_{\beta'\colon\alpha\rightarrow\beta'\in R}\text{count}(\alpha\rightarrow\beta')}
    $$
    \vspace{-1em}
    \begin{itemize}
        \item Similar to estimate word probabilities ($\rightarrow \beta$) given the document class ($\alpha$).
    \end{itemize}
    \pdfnote{
        Note that while we have trees, in PCFG, the rules are independent to each other, so we can consider the data as a set of rules.
    }
    \pdfnote{
        Now, the parameters we need to estimate are probabilities of each production rule.
        Rules with the same LHS follow a multinomial distribution.
        To make an anology to the multinomial naive bayes model we learned before,
        the LHS is like the document label, and the RHS rules are like words in the document.
        We want to estimate probabilities of rules given a LHS non-terminal, which is the same as estimating probabilities of words given a document class.
    }
\end{frame}

\begin{frame}
    {Parsing}
    Input: sentences, (P)CFG \\
    Output: derivations / parse trees (with scores/probabilities)

    Total number of parse trees for a sentence?

    Consider a minimal CFG:\\
    \begin{itemize}
        \item[] $X\rightarrow XX$
        \item[] $X\rightarrow \text{aardvark} | \text{abacus} | \ldots | \text{zyther}$
    \end{itemize}

    Given a string, \# of parse trees = \# of strings with balanced brackets\\
    \begin{itemize}
        \item[] $((w_1 w_2) (w_3 w_4))$, $(((w_1 w_2) w_3) w_4)$, ...
    \end{itemize}

    \# of strings with $n$ pairs of brackets:
    $$
    \text{Catalan number } C_n = \frac{1}{n+1}{2n\choose n}
    $$
\end{frame}

\begin{frame}
    {Chomsky normal form (CNF)}
    A CFG is in \textbf{Chomsky normal form} if every production rule takes one of the following forms:
    \begin{itemize}
        \item Binary non-terminal production: $A\rightarrow BC$ where $A, B, C\in N$.
        \item Unary terminal production: $A\rightarrow a$ where $A\in N, a\in\Sigma$.
    \end{itemize}

    Grammars in CNF produces \emph{binary} parse trees.

    Binarize a production rule: VP $\rightarrow$ VBD NP PP\\
    \begin{itemize}
        \item[] VP $\rightarrow$ VBD @VP-VBD
        \item[] @VP-VBD $\rightarrow$ NP PP
    \end{itemize}

    We assume the grammar are in CNF.
\end{frame}

\begin{frame}
    {Dynamic programming on the tree}
    $$
    p(t) = \underbrace{q(A\rightarrow B C)}_{\text{top rule}}
    \times \underbrace{p(t_B)}_{\text{left child}}
    \times \underbrace{p(t_C)}_{\text{right child}}
    $$
    \vspace{-1em}
    \begin{figure}
        \includegraphics[height=4cm]{figures/parse-2.png}
    \end{figure}
    \vspace{-1em}
    What are the variables when constructing a tree rooted at $A$ spanning $x_i,\ldots,x_j$?\\
    \begin{itemize}
        \item The production rule $A\rightarrow B C$
        \item The splitting point $s$: $B$ spans $x_i,\ldots,x_s$ and
            $C$ spans $x_{s+1}, \ldots, x_j$
    \end{itemize}
\end{frame}

\begin{frame}
    {The CYK algorithm}
    Notation: $\sT(i,j,X)$ is the set of trees with root node $X$ spanning $x_i,\ldots,x_j$ 

    Subproblem:
    $$\pi(i,j,X) = \max_{t\in\sT(i,j,X)} p(t)$$

    Base case:
    $$
    \pi(i,i,X) = \begin{cases}
        q(X\rightarrow x_i) & \text{if } X\rightarrow x_i \in R \\
        0 & \text{otherwise}
    \end{cases}
    $$

    Recursion:
    $$
    \pi(i,j,X) = \max_{\substack{Y,Z \in N\\ s\in\pc{i,\ldots,j-1}}}
    q(X\rightarrow YZ)\times \pi(i,s,Y) \times \pi(s+1, j, Z)
    $$

    Use backtracking to find the argmax tree.
\end{frame}


\begin{frame}
    {Bottom-up parsing}
    \begin{figure}
        \includegraphics[height=3cm]{figures/toy-cfg-2.png}
    \end{figure}
    \vspace{-2em}
    \begin{table}
        \begin{tabular}{c|p{1.2cm}|p{1.2cm}|p{1.2cm}|p{1.2cm}|p{1.2cm}}
        \hline
        & the & man & saw & the & dog \\
        0 & 0 & 1 & 2 & 3 & 4 \\
        \hline
        1 &&&&&\\[2ex]
        \hline
        2 &&&&&\\[2ex]
        \hline
        3 &&&&&\\[2ex]
        \hline
        4 &&&&&\\[2ex]
        \hline
    \end{tabular}
    \end{table}
\end{frame}

\begin{frame}
    {Variants of CYK}
    \textbf{Argmax}: find the most likely tree (analogous to Viterbi).
    $$
    \pi(i,j,X) = {\color{blue}\max}_{\substack{Y,Z\in N\\ s\in\pc{i,\ldots,j-1}}}
    q\p{X\rightarrow YZ} {\color{red}\times} \pi(i,s,Y) {\color{red}\times} \pi(s+1, j, Z)
    $$

    \textbf{Recognition}: does the string belong to the language?
    $$
    \pi(i,j,X) = {\color{blue}\vee}_{\substack{Y,Z\in N\\ s\in\pc{i,\ldots,j-1}}}
    \1\pb{X\rightarrow YZ\in R} {\color{red}\wedge} \pi(i,s,Y) {\color{red}\wedge} \pi(s+1, j, Z)
    $$

    \textbf{Marginalization}: what's the probability of the string being generated from the grammar? (the \textbf{inside algorithm})
    $$
    \pi(i,j,X) = {\color{blue}\sum}_{\substack{Y,Z\in N\\ s\in\pc{i,\ldots,j-1}}}
    q\p{X\rightarrow YZ} {\color{red}\times} \pi(i,s,Y) {\color{red}\times} \pi(s+1, j, Z)
    $$

\end{frame}

\begin{frame}
    {Summary}
    \begin{table}
        \renewcommand{\arraystretch}{1.7}
        \begin{tabular}{p{3cm}p{2cm}p{3cm}p{2cm}}
            \toprule
            & NB & HMM & PCFG \\
            \midrule
            output structure & category & sequence & tree \\
            learning & & & \\
            decoding & bruteforce & Viterbi & CKY\\
            marginalization & & \makecell{$p(y_i \mid x)$,\\$ p(y_i, y_{i-1}\mid x)$} & $p(i, j, N \mid x)$ \\
            unsupervised learning & & & \\
            \bottomrule
        \end{tabular}
    \end{table}
\end{frame}

\section{Discriminative parsing}

\begin{frame}
    {CRF for trees}
    \emph{Input}: sequence of words $x=(x_1, \ldots, x_n)$\\
    \emph{Output}: parse tree $y\in\sT(x)$\\
    \emph{Model}: decompose by production rules 
    $$
    p(y\mid x;\theta) \propto \prod_{(r,s)} \psi(r, s\mid x; \theta)
    $$
    \vspace{-1em}
    \begin{itemize}
        \item $r$: production rule
        \item $s$: start, split, end indices of the rule $r$
    \end{itemize}
    
    \begin{tikzpicture}
        \node {\includegraphics[width=3cm]{figures/toy-tree}};
    \end{tikzpicture}
\end{frame}

\begin{frame}
    {CRF parsing}
    Potential functions:
    \begin{align*}
        \psi(r, s\mid x; \theta) &= \exp\p{\theta\cdot\phi(r,s,x)}\\
        \prod_{(r,s)\in\sT(x)}\psi(r, s\mid x; \theta) &= \exp\p{\sum_{(r,s)\in\sT(x)}\theta\cdot\phi(r,s,x)}\\
    \end{align*}

    Learning: MLE\\
    \begin{enumerate}
        \item Compute the partition function by the inside algorithm
        \item Call autograd to compute the gradient (backpropagation)
    \end{enumerate}

    Inference: CYK
\end{frame}

\begin{frame}
    {Limitations of PCFG}
    \begin{figure}
        \includegraphics[height=5cm]{figures/parse-1}
    \end{figure}
    Limited lexical information
    \pdfnote{
        When we decide the PP attachment, there is no conditioning on other words in the sentence.
        And it mainly boils down to the probabilities of NP -> NP PP and VP -> VP PP.
    }
\end{frame}

\begin{frame}
    {Lexicalized PCFG}
    Attach the ``head'' (most important child in a rule) of the span to each non-terminal
    \begin{figure}
        \includegraphics[height=7cm]{figures/lex-pcfg}
    \end{figure}
    \pdfnote{
        Head is obtained by rules or from annotation.
    }
    \pdfnote{
        What an obvious drawback here? More grammar rules.
    }
\end{frame}

\begin{frame}
    {Features}
    Easy to incorporate lexical information in features!
    $$
    \text{local score} = \theta \cdot \phi(\text{VP $\rightarrow$ VBD NP}, (5, 6, 8), \text{...averted financial disaster...})
    $$
    \begin{figure}
        \includegraphics[height=5cm]{figures/parse-feature}
        \caption{Less grammar, more features. [Hall+ 14]}
    \end{figure}
\end{frame}

\begin{frame}
    {Neural CRF parser}
    \begin{figure}
        \includegraphics[height=4.5cm]{figures/neural-crf}
        \caption{Neural CRF Parsing. [Durrett+ 15]}
    \end{figure}
    \begin{itemize}
        \item $f_w$: lexical features
        \item $f_o$: rule features
        \item $h^TWf_o$: interaction between lexical and rule features
    \end{itemize}
\end{frame}

\begin{frame}
    {Evaluation}
    \begin{align*}
        \text{recall} &= \frac{\# \text{correct constituents}}{\# \text{total constituents in gold trees}}\\\\
        \text{precision} &= \frac{\# \text{correct constituents}}{\# \text{total constituents in predicted trees}} \\\\
        \text{F1} &= \frac{2\times\text{precision}\times\text{recall}}{\text{precision}+\text{recall}}
    \end{align*}
    \begin{itemize}
        \item Constituent: $(i, j, X)$
        \item Labeled F1: the non-terminal node label must be correct
        \item Unlabeled F1: just consider the tree structure
    \end{itemize}
\end{frame}

\begin{frame}
    {Example}
    \begin{figure}
        \begin{subfigure}[b]{0.45\textwidth}
            \includegraphics[width=\textwidth]{figures/parse-1.png}
            \subcaption{Gold.}
        \end{subfigure}
        \begin{subfigure}[b]{0.45\textwidth}
            \includegraphics[width=\textwidth]{figures/parse-2.png}
            \subcaption{Predicted.}
        \end{subfigure}
    \end{figure}
\end{frame}

\end{document}
