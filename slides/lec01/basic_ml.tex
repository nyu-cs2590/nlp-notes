\documentclass[usenames,dvipsnames,notes,11pt,aspectratio=169]{beamer}
\usepackage{ifthen}
\usepackage{xcolor}
\usepackage{pgfplots}
\usepackage{amsmath}
\usepackage{centernot}
\usepackage{pifont}
\usepackage{tabularx}
\usepackage{makecell}
\usepackage{cuted}
\usepackage{booktabs}
\usepackage{array}
\usepackage{setspace}
\usepackage{CJKutf8}
\usepackage{textcomp}


%\usepackage{pgfpages}
%\setbeameroption{show notes on second screen}

\pgfplotsset{compat=1.17,
    every axis/.append style={
            font=\large,
            line width=1pt,
            tick style={line width=0.8pt}}}

\input ../beamer-style
\input ../std-macros
\input ../macros

\AtBeginSection[]
{
    \begin{frame}
        \frametitle{Table of Contents}
        \tableofcontents[currentsection]
    \end{frame}
}



\title[CSCI-GA.2590]{Machine Learning Basics}
\author[He He]{He He
}
\institute[NYU]{New York University}
\date{September 8, 2021}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\section{Generalization}

\begin{frame}
    {Rule-based approach}
\begin{figure}
\includegraphics[height=0.7\textheight]{figures/geron-fig1-1}
    \caption{{Fig 1-1 from \emph{Hands-On Machine Learning with Scikit-Learn and TensorFlow} by Aurelien Geron (2017).}}
\end{figure}
\end{frame}

\begin{frame}
    {Machine learning approach}
\begin{figure}
\includegraphics[height=0.7\textheight]{figures/geron-fig1-2}
    \caption{{Fig 1-2 from \emph{Hands-On Machine Learning with Scikit-Learn and TensorFlow} by Aurelien Geron (2017).}}
\end{figure}
\end{frame}

\begin{frame}
    {Example: spam filter}
    
    \begin{itemize}
        \itemsep1em
        \item Rules
            \begin{itemize}
                \item[] Contains ``Viagra''
                \item[] Contains ``Rolex''
                \item[] Subject line is all caps
                \item[] ... 
            \end{itemize}
        \item Learning from data
            \begin{enumerate}
                \item Collect emails labeled as spam or non-spam 
                \item (Design features) 
                \item Learn a predictor
            \end{enumerate}
    \end{itemize}
    \medskip
    Pros and cons?
\end{frame}

\begin{frame}
    {Keys to success}
    \begin{itemize}
        \itemsep1em
        \item Availability of large amounts of (annotated) data
            \begin{itemize}
                \item[] Scraping, crowdsourcing, expert annotation
            \end{itemize}
        \item \blue{Generalize} to unseen samples (test set)
            \begin{itemize}
                \item Assume that there is a (unknown) data generating distribution: $\sD$ over $\sX\times\sY$
               \item Training set: $m$ samples from $\sD$ $\pc{(x^{(i)}, y^{(i)})}_{i=1}^m$
               \item Learn model $h\colon \sX \rightarrow \sY$
               \item Goal: $\text{minimize}\quad 
                   \mathbb{E}_{(x,y)\sim\sD}\pb{\text{error}(h,x,y)}$ (estimated on the test set)
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    {Empirical risk minimization (ERM)}
    \begin{itemize}
        \itemsep1em
        \item Our goal is to minimize the expected loss (\textbf{risk}), but it cannot be computed (why?).
        \item How can we estimate it?
    \pause

\item Minimize the {average loss} (\textbf{empirical risk}) on the training set %over $\sH$
    $$
    \min_h \frac{1}{m}\sum_{i=1}^m \text{error}(h, x^{(i)}, y^{(i)})
    $$

\item In the limit of infinite samples, empirical risk converges to risk (LLN).
        \item Given limited data though, can we generalize by ERM?
    \end{itemize}
\end{frame}

%\begin{frame}
%    {Error decomposition}
%\end{frame}

\begin{frame}
    {Overfitting vs underfitting}
    \begin{itemize}
        \item Trivial solution to (unconstrained) ERM: \red{memorize} the data points
        \item Need to extrapolate information from one part of the input space to unobserved parts!
    \end{itemize}

    \vspace{10em}
    \pause
    \begin{itemize}
        \item Constrain the prediction function to a subset, i.e.\ a \textbf{hypothesis space} $h\in \sH$.
        \item Trade-off between complexity of $\sH$ (approximiation error) and estimation error
        \item Question for us: how to choose a good $\sH$ for certain domains
    \end{itemize}
\end{frame}

\section{Optimization}

\begin{frame}
    {Overall picture}
    \begin{enumerate}
        \itemsep2em
        \item Obtain training data $D_{\text{train}}=\pc{(x^{(i)}, y^{(i)})}_{i=1}^n$.
        \item Choose a loss function $L$ and a hypothesis class $\sH$ (domain knowledge).
        \item Learn a predictor by minimizing the empirical risk (optimization).
    \end{enumerate}
\end{frame}

\begin{frame}
    {Gradient descent}
    \begin{itemize}
        \item The gradient of a function $F$ at a point $w$ is the direction of fastest increase in the function value
        \item To minimze $F(w)$, move in the opposite direction
        $$w \leftarrow w - \eta\nabla_w F(w)$$
        \item Converge to a local minimum (also global minimum if $F(w)$ is \textbf{convex}) with carefully chosen step sizes
    \end{itemize}
    \vspace{9em}
\end{frame}

\begin{frame}
    {Convex optimization (unconstrained)}
    \begin{itemize}
        \item A function $f\colon \BR^d \rightarrow \BR$ is convex if for all $x,y \in \BR^d$ and $\theta \in [0,1]$ we have
            $$
            f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta) f(y)  \;.
            $$
        \vspace{6em}
        \item $f$ is concave if $-f$ is convex.\\
        \item Locally optimal points are also globally optimal.
        \item For unconstrained problems, $x$ is optimal iff $\nabla f(x) = 0$.
    \end{itemize}
\end{frame}

\begin{frame}
    {Stochastic gradient descent}
    \begin{itemize}
        \item \textbf{Gradient descent (GD)} for ERM
            $$
            w \leftarrow w - \eta\nabla_w \underbrace{\sum_{i=1}^n L(x^{(i)}, y^{(i)}, f_w)}_{\textstyle{\text{training loss}}}
            $$
            \pause
        \item \textbf{Stochastic gradient descent (SGD)}: take noisy but faster steps\\
            \begin{align*}
                \text{For each } &(x, y) \in D_{\text{train}}:\\
                &w \leftarrow w - \eta\nabla_w \underbrace{L(x, y, f_w)}_{\textstyle{\text{example loss}}}
            \end{align*}
    \end{itemize}
\end{frame}

\begin{frame}
    {GD vs SGD}
    \begin{figure}
        \caption{Minimize $1.25(x + 6)^2 + (y - 8)^2$}
        \includegraphics[height=5cm]{figures/gd-vs-sgd}
    \end{figure}
    (\small{Figure from ``Understanding Machine Learning: From Theory to Algorithms''.})
\end{frame}

\begin{frame}
    {Stochastic gradient descent}
    \begin{itemize}
        \itemsep1em
        \item Each update is efficient in both time and space
        \item Can be slow to converge 
        \item Popular in large-scale ML, including non-convex problems
        \item In practice, 
            \begin{itemize}
                \item[] Randomly sample examples.
                \item[] Fixed or diminishing step sizes, e.g. $1/t$, $1/\sqrt{t}$.
                \item[] Stop when objective does not improve.
            \end{itemize}
    \end{itemize}
\end{frame}

\section{Loss functions}

\begin{frame}
    {Zero-one loss}
    \begin{itemize}
        \item Binary classification: $y\in\pc{+1, -1}$.
            \begin{itemize}
                \item Model: $f_w\colon \sX \rightarrow \bR$ parametrized by $w \in \bR^d$.
                \item Output prediction: $\text{sign}(f_w(x))$.
            \end{itemize}
        \item Zero-one (0-1) loss
            \begin{align}
                L(x, y, f_w) = \1\pb{\text{sign}(f_w(x)) = y} 
                %= \1\pb{\underbrace{yf_w(x)}_{\textstyle\text{margin}}\le 0}
                = \1\pb{{yf_w(x)} \le 0}
            \end{align}
    \end{itemize}
    \begin{figure}
        \includegraphics[height=4cm]{figures/loss.Zero_One.png}
    \end{figure}
    \pause
    \red{Not feasible for ERM}
\end{frame}

\begin{frame}
    {Hinge loss}
    $$
    L(x,y,f_w) = \max(1-yf_w(x), 0)
    $$
    \begin{figure}
        \includegraphics[height=4cm]{figures/loss.Zero_One.Hinge.png}
    \end{figure}
    \begin{itemize}
        \item Loss is zero if margin is larger than 1
        \item Not differentiable at $\text{margin}=1$
        \item Subgradient: $\pc{g\colon f(x) \ge x_0 + g^T(x-x_0)}$
    \end{itemize}
\end{frame}

\begin{frame}
    {Logistic loss}
    $$
    L(x,y,f_w) = \log(1+e^{-yf_w(x)})
    $$
    \begin{figure}
        \includegraphics[height=4cm]{figures/loss.Zero_One.Hinge.Logistic.png}
    \end{figure}
    \begin{itemize}
        \item Differentiable
        \item Always wants more margin (loss is never 0)
    \end{itemize}
\end{frame}

\begin{frame}
    {Summary}
    \begin{itemize}
        \itemsep2em
        \item Bias-complexity trade-off: choose hypothesis class based on prior knowledge
        \item Learning algorithm: empirical risk minimization
        \item Optimization: stochastic gradient descent
    \end{itemize}
\end{frame}

\end{document}
